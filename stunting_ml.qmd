
## Import dataset

```{r load-packages, warning=FALSE}
pacman::p_load(
  tidyverse, # Data management and visualisation
  rio, # Import and export dataset
  here, # File management
  finalfit, # Labelling
  skimr, # Descriptive stats
  ggstatsplot, # Exploratory data analysis
  gtsummary, # Table summaries
  rsample, # Data split
  parsnip, # Fit models
  workflows, # Model workflow
  workflowset, # Combination of workflow components
  yardstick, # Model performance
  tune, # Performance metrics using resample
  DALEX, # Explainer
  DataExplorer, # Automated data exploration
  gtExtras, # Summary tables
  themis, # Subsampling
  modelDown, # Explainable AI report
  rstanarm, # Model packages
  ranger, # Random forest
  foreign, #Import spss dataset with factor variables loaded correctly (NB: You can use rio if you set haven to false and set use.value.labels = TRUE)
  recipeselectors,
  vip,
  recipes, # Feature engineering
  chatgpt
)
```


```{r import-dataset}
df <- import(here("data", "df_wrangled.RData"))
```

## Feature engineering with recipes

```{r data-budget}
# Split data into training and testing
set.seed(123)

df_split <- initial_split(df_clean, prop = 0.70, strata = stunting)

df_split
```


```{r extract-training-testing-data}
# Training dataset
df_train <- training(df_split)

# Testing dataset
df_test <- testing(df_split)
```



```{r rf-feature-selection}
# Using random forest for feature selection
base_model <- 
  rand_forest(mode = "classification") |>
  set_engine("ranger", importance = "permutation")

df_rec <- df_train |>
  recipes::recipe(stunting ~ .) |>
  recipeselectors::step_select_vip(
    all_predictors(),
    model = base_model,
    outcome = "stunting",
    top_p = 10
  )
```



```{r preprocessing}
# Preprocessing steps using recipes
df_rec <- 
  recipe(stunting ~ ., data = df_train) |>
  step_corr(threshold = 0.80) |>
  step_dummy(all_nominal_predictors())
```


```{r upsample}
df_upsample <- 
  df_rec |>
  step_upsample(stunting)
```


## Modelling with parsnip and workflow

```{r create-models}
# Logistic regression
logistic_spec <- logistic_reg() |>
  set_engine("stan")

# Linear discriminant analysis
lda_spec <- discrim_linear() |>
  set_engine("sparsediscrim")

# Naves Bayes
nb_spec <- naive_Bayes() |>
  set_engine("naivebayes") |>
  set_mode("classification")

# Support Vector Machine
svm_spec <- svm_linear() |>
  set_engine("kernlab") |>
  set_mode("classification")

# Extreme Gradient Boosting
xbgboost_spec <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("classification")

# Neural network
nn_spec <- bag_mlp() |>
  set_engine("nnet") |>
  set_mode("classification")

# Random forest
rf_spec <- rand_forest(trees = 1000, min_n = 5) |> 
  set_engine("ranger") %>% 
  set_mode("classification")

# K-Nearest Neighbours
knn_spec <- nearest_neighbor() |>
  set_engine("kknn") |>
  set_mode("classification")

# Automatic machine learning
automl_spec <- auto_ml() |>
  set_engine("h20") |>
  set_mode("classification")
```


```{r model-workflow}
# Example model workflow
logit_wflow <- 
  workflow() |>
  add_model(logistic_spec) |>
  add_recipe(df_rec)
```


```{r workflow-set}
# Example workflow set
# df_wflowset <- workflow_set(preproc = location, models = list(lm = lm_model))
```


```{r fit-models}
# Example of model fitting
logit_fit <- fit(logit_wflow, df_train)
```


## Model performance with yardstick

```{r binary-performance}
# Confusion matrix
conf_mat()

# Classification metrics
classification_metrics <- metric_set(accuracy, mcc, f_meas)

# ROC and AUC
roc_auc()
roc_curve()
```


```{r resampling-methods}
# Resampling methods for model performance
set.seed(123)
df_folds <- vfold_cv(df_train, v = 10)

# Example model fit
rf_res <- 
  rf_wflow |>
  fit_resamples(df_fold)

# Performance metrics
collect_metrics(rf_res)
```



```{r predict-using-workflow}
# Example of prediction using workflow
# predict(logit_fit, df_test)
```

